{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary of characters and mapping to/from integers\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# Build Dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes and Notes\n",
    "* `b2 * 0`  and `W2 * 0.1` since we want the logits closer to 0, so we remove the bias and scale down W2\n",
    "* You can get dead neurons in activation functions. This can be from bad luck, but even if they don't die, you can saturate the activation and significantly slow down learning.\n",
    "    * If you see this oversaturation, this will be beacuse values that are too extreme are being fed into the activation\n",
    "    * In our case we can scale down `b1 * 0.01` and `W1 * 0.2`\n",
    "* In deeper networks (i.e 50), a bad initialization can stop a network from learning entirely\n",
    "* Initialization used to be a very big problem on large networks, but new tools have loosening some of these constraints\n",
    "    * Initializations today rely on `kaiming_normal`\n",
    "    * For our `W1` it's the gain for our non-linearlity `tanh` = (5/3) * the first dimension of `W1`\n",
    "    * Skip connections, Batch and Layer Norm, better optimizers instead of just SGD like Adam are new tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "# MLP revistited\n",
    "\n",
    "n_emb = 10 # Dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # Number of neirons in the hidden layer of the MLP\\\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_emb),                generator=g)\n",
    "W1 = torch.randn((n_emb * block_size, n_hidden),    generator=g) * (5/3)/(n_emb * block_size)**0.5 # 0.2\n",
    "# b1 = torch.randn((n_hidden),                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),            generator=g) * 0.01\n",
    "b2 = torch.randn((vocab_size),                      generator=g) * 0\n",
    "\n",
    "# Batch norm trainable parameters\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.zeros((1, n_hidden))\n",
    "\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # total params\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Norm\n",
    "* We want to make sure that the values of the hidden layer pre activation are roughly unit Gaussian. Too close to zero and the tanh does nothing. To big and the tanh does too much\n",
    "* So the idea is to just standardize the hidden state to make it unit Gaussian\n",
    "* But you want the distribution to be around the Gaussian but not exactly. So we add scale and shift parameters ***LOOK UP***\n",
    "* In the denominator when the value is being scaled by the std, add a very small $\\epsilon$ so that you don't divide by zero\n",
    "* Whenever you have a batchnorm layer, it ends up subtracting out previous bias in the weight layers. In our case we can remove `b1` since it won't actually do anything and will be wasteful\n",
    "* BatchNorm \"just works\" but it can cause a lot of bugs due to the coupling of examples, so try using other more stable normalization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3147\n",
      "  10000/ 200000: 2.1984\n",
      "  20000/ 200000: 2.3375\n",
      "  30000/ 200000: 2.4359\n",
      "  40000/ 200000: 2.0119\n",
      "  50000/ 200000: 2.2595\n",
      "  60000/ 200000: 2.4775\n",
      "  70000/ 200000: 2.1020\n",
      "  80000/ 200000: 2.2788\n",
      "  90000/ 200000: 2.1862\n",
      " 100000/ 200000: 1.9474\n",
      " 110000/ 200000: 2.3010\n",
      " 120000/ 200000: 1.9837\n",
      " 130000/ 200000: 2.4523\n",
      " 140000/ 200000: 2.3839\n",
      " 150000/ 200000: 2.1987\n",
      " 160000/ 200000: 1.9733\n",
      " 170000/ 200000: 1.8668\n",
      " 180000/ 200000: 1.9973\n",
      " 190000/ 200000: 1.8347\n"
     ]
    }
   ],
   "source": [
    "# Same optimization as MLP 1\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    \n",
    "    # Minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size, ), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[Xb] # Embed the characters into the vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # Concat the vectors\n",
    "    # Linear Layer\n",
    "    hpreact = embcat @ W1 # + b1 # Hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    #-------------------------------------------------------------------------\n",
    "    bnmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi = hpreact.std(0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias # Normalize the batch and add scale and shift params\n",
    "    with torch.no_grad(): # Keep track of runnin batchnorm mean and std\n",
    "        bnmean_running  = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "        bnstd_running  = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    #-------------------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # Hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # Track stats\n",
    "    if i % 10000 == 0: # Print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 200])\n",
      "torch.Size([1, 200])\n",
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "print(hpreact.shape)\n",
    "# Mean and std of each of the records of the minibatch\n",
    "print(hpreact.mean(0, keepdim=True).shape)\n",
    "print(hpreact.std(0, keepdim=True).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial loss is way too high. This indicates the setup is poor. We can calculate an estimated initial loss, by considering the loss function and the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2958)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At initialiation, we would expect that the probability of every character to be the same, so 1/27\n",
    "# So we would take that prob and negative log it to get our loss\n",
    "-torch.tensor(1/27.0).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@torch.no_grad()` Basically tells the function beneath it that any tensors will not need to do gradient tracking. It improves efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the batch norm at the end of the training\n",
    "# Basically clamping the estimated batched values so that you can do inference 1 item at a time\n",
    "# But no one wants this extra step. So these values are kept as a running mean and std during training\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # measure the mean/std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnstd = hpreact.std(0, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to check we would see that bnmean is roughly equal to bnmean_ running and the same for the stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0667834281921387\n",
      "val 2.1049110889434814\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # This decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Loss\n",
    "\n",
    "* Original\n",
    "    * train 2.125\n",
    "    * val   2.168\n",
    "* Fix softmax confidently wrong\n",
    "    * train 2.069\n",
    "    * val   2.131\n",
    "* Fix tanh layer too saturated init\n",
    "    * train 2.036\n",
    "    * val   2.103\n",
    "* Add BatchNorm Layer *Doesn't make a difference in such a small network*\n",
    "    * train 2.067\n",
    "    * val   2.104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmah.\n",
      "amille.\n",
      "khirmrex.\n",
      "taty.\n",
      "skanden.\n",
      "jazonel.\n",
      "den.\n",
      "arci.\n",
      "aqui.\n",
      "ner.\n",
      "kentzieiivia.\n",
      "legy.\n",
      "dham.\n",
      "jorn.\n",
      "quintis.\n",
      "lilea.\n",
      "jadii.\n",
      "waythoniearynix.\n",
      "kaeliigh.\n",
      "boe.\n"
     ]
    }
   ],
   "source": [
    "# Sample from the model\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1,-1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "Let's pytorchify the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # Params\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # Buffers (running estimate)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Forward Pass\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True) # batch mean\n",
    "            xvar = x.var(0, keepdim=True) # batch var\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x-xmean)/torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        # Update Buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46497\n"
     ]
    }
   ],
   "source": [
    "n_emb = 10\n",
    "n_hidden = 100\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_emb), generator=g)\n",
    "layers = [\n",
    "    Linear(n_emb * block_size, n_hidden), Tanh(),\n",
    "    Linear(          n_hidden, n_hidden), Tanh(),\n",
    "    Linear(          n_hidden, n_hidden), Tanh(),\n",
    "    Linear(          n_hidden, n_hidden), Tanh(),\n",
    "    Linear(          n_hidden, n_hidden), Tanh(),\n",
    "    Linear(          n_hidden, vocab_size)\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # last layer: make less confident\n",
    "    layers[-1].weight += 0.1\n",
    "    # all other layers: apply gain\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for  p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.4828\n"
     ]
    }
   ],
   "source": [
    "# same optimization as before\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size, ), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # Batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    x = emb.view(emb.shape[0], -1) # Concat the vectors\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb) # loss function\n",
    "\n",
    "    # Backward Pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad() # AFTER_DEBUG: would take out retain grpah\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # Track Stats\n",
    "    if i%10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    break # Obviusly take out after Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Initialization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 (      Tanh): mean -0.02, saturated: 0.75%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-d1837ea95c6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layer %d (%10s): mean %+.2f, saturated: %.2f%%'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mlegends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'layer {i} ({layer.__class__.__name__})'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize histogram Tanh analysis\n",
    "import pandas as pd\n",
    "plt.figure(figsize=(20,4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]):\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out\n",
    "        print('layer %d (%10s): mean %+.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
    "        hy, hx = torch.histc(t)\n",
    "        plt.plot(hx[:-1].detach, hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__})')\n",
    "plt.legend(legends)\n",
    "plt.title('Activation Distributions');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
